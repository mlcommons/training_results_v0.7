# Copyright 2018 Google. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Train NMT with low level API."""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import six
import threading
import time
from six.moves import queue as Queue

import tensorflow.compat.v1 as tf
from REDACTED.tensorflow.contrib import cluster_resolver as contrib_cluster_resolver

from REDACTED import rewriter_config_pb2
from REDACTED.tensorflow.python.tpu import tpu
from REDACTED.tensorflow.python.tpu import tpu_feed
from REDACTED.tensorflow.python.tpu import tpu_function
from REDACTED.tensorflow.python.tpu import training_loop
from REDACTED.tensorflow.python.tpu.ops import tpu_ops
from REDACTED.tensorflow.python.data.util import nest as data_nest
from REDACTED.tensorflow.python.framework import graph_io
from REDACTED.mlp_log import mlp_log
from REDACTED.nmt import metric
from REDACTED.nmt.utils import iterator_utils


_INITIAL_LOSS = 1e7
_STOP = -1
_ITEM = 1


def wrap_computation_in_while_loop(op_fn,
                                   n,
                                   host_name,
                                   include_induction_variable=False):
  """Wraps the ops generated by `op_fn` in tf.while_loop."""

  def computation(i):
    ops = op_fn(i) if include_induction_variable else op_fn()
    if not isinstance(ops, list):
      ops = [ops]
    with tf.control_dependencies(ops):
      return i + 1

  with tf.device(device_for_host(host_name)):
    return tf.while_loop(
        lambda i: tf.less(i, n),
        computation, [tf.constant(0)],
        parallel_iterations=1)


def get_resolver(hparams):
  if hparams.master:
    return contrib_cluster_resolver.TPUClusterResolver(hparams.master)
  elif hparams.tpu_name:
    return contrib_cluster_resolver.TPUClusterResolver(hparams.tpu_name)
  else:
    return None


def get_host(resolver, hparams, host_id=0):
  if resolver is None:
    return "/replica:0/task:0"
  elif hparams.master == "local":
    return "/job:localhost/replica:0/task:0"
  else:
    job_name = resolver.get_job_name() or hparams.tpu_job_name or "tpu_worker"
    return "/job:%s/task:%d" % (job_name, host_id)


def device_for_host(host_name):
  return host_name + "/device:CPU:0"


def device_for_tpu_core(host_name, core=0):
  return host_name + "/device:TPU_REPLICATED_CORE:%d" % core


class LowLevelRunner(object):
  """Run Train via direct session.run calls."""

  def __init__(self,
               hparams,
               train_iterations,
               eval_steps,
               per_host_v1=False):
    tf.logging.info("TrainLowLevelRunner: constructor")

    self.feature_structure = {}
    self.eval_feature_structure = {}
    self.loss = None
    self.infeed_queue = []
    self.eval_infeed_queue = []
    self.enqueue_ops = []
    self.eval_enqueue_ops = []
    self.dataset_initializer = []
    self.eval_dataset_initializer = []
    self.is_local = ((hparams.master == "") and (hparams.tpu_name is None))
    self.per_host_v1 = per_host_v1
    self.iterations = train_iterations
    self.eval_steps = eval_steps
    self.outfeed_tensors = []
    self.outfeed_names = []
    self.dequeue_ops = []
    self.predictions = {}
    self.sess = None
    self.graph = tf.Graph()
    self.hparams = hparams
    self.num_hosts = hparams.num_shards // hparams.num_shards_per_host
    with self.graph.as_default():
      self.tpu_init = [tpu.initialize_system()]
      self.tpu_shutdown = tpu.shutdown_system()

    self.resolver = get_resolver(hparams)
    session_config = tf.ConfigProto(
        allow_soft_placement=True,
        isolate_session_state=True,
        operation_timeout_in_ms=600 * 60 * 1000,
        graph_options=tf.GraphOptions(
            rewrite_options=rewriter_config_pb2.RewriterConfig(
                disable_meta_optimizer=True)))

    if self.hparams.tpu_name is None:
      master = self.hparams.master
    else:
      cluster_spec = self.resolver.cluster_spec()
      if cluster_spec:
        session_config.cluster_def.CopyFrom(cluster_spec.as_cluster_def())
      master = self.resolver.get_master()
    self.sess = tf.Session(master, graph=self.graph, config=session_config)
    self.sess.run(self.tpu_init)

  def initialize(self, input_fn, eval_input_fn, params):
    """Initialize all the things required for training."""
    tf.logging.info("TrainLowLevelRunner: initialize method")

    def get_enqueue_ops_fn(host_id):
      """Generate the enqueue ops graph function."""
      device = device_for_host(get_host(self.resolver, self.hparams, host_id))
      if host_id in range(0, self.hparams.num_infeed_workers * 2, 2):
        with tf.device(device):
          params["batch_size"] = self.hparams.batch_size
          params["dataset_num_shards"] = self.hparams.num_infeed_workers
          params["dataset_index"] = host_id // 2
          output = input_fn(params)
          if not self.hparams.use_synthetic_data:
            iterator = output.make_initializable_iterator()
            self.dataset_initializer.append(iterator.initializer)
            if host_id == 0:
              f = iterator.get_next()
              self.feature_structure["features"] = {
                  k: tf.zeros_like(f[k]) for k in f
              }
          else:
            self.feature_structure["features"] = output
          self.feature_structure["core_id"] = tf.constant([1], tf.int32)

      def enqueue_ops_fn(idx):
        """Enqueue ops function for one host.."""
        with tf.device(device):
          sharded_inputs = []
          start_idx = 0
          if host_id in range(0, self.hparams.num_infeed_workers * 2, 2):
            core_id = tf.constant(
                host_id * self.hparams.num_shards_per_host,
                shape=[1],
                dtype=tf.int32)
            if self.hparams.use_synthetic_data:
              features = output
            else:

              def true_fn():
                return iterator.get_next()

              def false_fn():
                return {
                    k: tf.zeros_like(self.feature_structure["features"][k])
                    for k in self.feature_structure["features"]
                }

              features = tf.cond(
                  tf.equal(idx % self.hparams.num_infeed_workers, host_id // 2),
                  true_fn, false_fn)
            sharded_inputs.append(
                data_nest.flatten({
                    "features": features,
                    "core_id": core_id
                }))
            start_idx = 1
          for i in range(start_idx, self.hparams.num_shards_per_host):
            sharded_inputs.append(
                data_nest.flatten({
                    "features": {
                        k: tf.zeros_like(self.feature_structure["features"][k])
                        for k in self.feature_structure["features"]
                    },
                    "core_id":
                        tf.constant(
                            host_id * self.hparams.num_shards_per_host + i,
                            shape=[1],
                            dtype=tf.int32)
                }))
        infeed = tpu_feed.InfeedQueue(
            number_of_tuple_elements=len(sharded_inputs[0]))
        self.infeed_queue.append(infeed)

        def tpu_ordinal_fn(shard_index_in_host):
          return shard_index_in_host % self.hparams.num_shards_per_host

        return infeed.generate_enqueue_ops(
            sharded_inputs, tpu_ordinal_function=tpu_ordinal_fn)

      return enqueue_ops_fn

    def get_eval_enqueue_ops_fn(host_id):
      """Generate the enqueue ops graph function."""

      params["dataset_num_shards"] = self.num_hosts
      params["dataset_index"] = host_id
      with tf.device(
          device_for_host(get_host(self.resolver, self.hparams, host_id))):
        dataset = eval_input_fn(params)
        iterator = dataset.make_initializable_iterator()
        self.eval_dataset_initializer.append(iterator.initializer)

        def enqueue_ops_fn():
          """Enqueue ops function for one host."""
          per_host_sharded_inputs = []
          control_deps = []
          for _ in range(self.hparams.num_shards_per_host):
            with tf.control_dependencies(control_deps):
              features = iterator.get_next()
            self.eval_feature_structure["features"] = features
            flattened_inputs = data_nest.flatten(self.eval_feature_structure)
            control_deps.extend(flattened_inputs)
            per_host_sharded_inputs.append(flattened_inputs)

          infeed = tpu_feed.InfeedQueue(
              number_of_tuple_elements=len(per_host_sharded_inputs[0]))
          self.eval_infeed_queue.append(infeed)

          def tpu_ordinal_fn(shard_index_in_host):
            return shard_index_in_host % self.hparams.num_shards_per_host

          return infeed.generate_enqueue_ops(
              per_host_sharded_inputs, tpu_ordinal_function=tpu_ordinal_fn)

        return enqueue_ops_fn

    with self.graph.as_default():
      if self.iterations > 0:
        for i in range(self.num_hosts):
          self.enqueue_ops.append(
              wrap_computation_in_while_loop(
                  get_enqueue_ops_fn(i),
                  n=self.iterations,
                  host_name=get_host(self.resolver, self.hparams, i),
                  include_induction_variable=True))

      if self.eval_steps > 0:
        for i in range(0, self.num_hosts):
          self.eval_enqueue_ops.append(
              wrap_computation_in_while_loop(
                  get_eval_enqueue_ops_fn(i),
                  n=self.eval_steps,
                  host_name=get_host(self.resolver, self.hparams, i)))
      init_tables = tf.tables_initializer()

    self.sess.run(init_tables)
    self.sess.run(self.dataset_initializer)

  def build_model(self, model_fn, params):
    """Build the TPU model and infeed enqueue ops."""
    tf.logging.info("TrainLowLevelRunner: build_model method")

    def tpu_train_step(loss):
      """Generate the TPU graph."""
      del loss
      values = self.infeed_queue[0].generate_dequeue_op(tpu_device=0)
      unflattened_inputs = data_nest.pack_sequence_as(self.feature_structure,
                                                      values)
      features = unflattened_inputs["features"]
      core_id = unflattened_inputs["core_id"]
      new_features = {}
      for k in features:
        s = features[k].shape.as_list()
        s = [self.hparams.num_shards, s[0] // self.hparams.num_shards] + s[1:]
        new_features[k] = tf.squeeze(
            tf.gather(
                tf.reshape(tpu_ops.cross_replica_sum(features[k]), s), core_id),
            [0])

      estimator_spec = model_fn(new_features, None, tf.estimator.ModeKeys.TRAIN,
                                params)
      loss, train_op = estimator_spec.loss, estimator_spec.train_op
      with tf.control_dependencies([train_op]):
        return tf.identity(loss)

    @tpu_function.on_device_training_loop
    def train_loop():
      return training_loop.repeat(self.iterations, tpu_train_step,
                                  [_INITIAL_LOSS])

    def tpu_eval_step():
      """Generate the TPU graph."""
      values = self.eval_infeed_queue[0].generate_dequeue_op(tpu_device=0)
      unflattened_inputs = data_nest.pack_sequence_as(
          self.eval_feature_structure, values)
      features = unflattened_inputs["features"]
      estimator_spec = model_fn(features, None, tf.estimator.ModeKeys.PREDICT,
                                params)
      for k, v in six.iteritems(estimator_spec.predictions):
        self.outfeed_names.append(k)
        self.outfeed_tensors.append(v)

      with tf.device(
          device_for_tpu_core(get_host(self.resolver, self.hparams))):
        outfeed_enqueue_ops = tpu_ops.outfeed_enqueue_tuple(
            self.outfeed_tensors)
      with tf.control_dependencies([outfeed_enqueue_ops]):
        return tf.no_op()

    @tpu_function.on_device_training_loop
    def eval_loop():
      if self.eval_steps > 0:
        return training_loop.repeat(self.eval_steps, tpu_eval_step, [])
      else:
        return tf.no_op()

    def train_eval_step():
      with tf.control_dependencies(train_loop()):
        return eval_loop()

    def train_eval_loop():
      return training_loop.repeat(self.hparams.max_train_epochs,
                                  train_eval_step, [])

    def create_dequeue_ops(host_id):
      """Create outfeed dequeue ops."""
      dequeue_ops = []
      tensor_dtypes = []
      tensor_shapes = []
      for v in self.outfeed_tensors:
        dequeue_ops.append([])
        tensor_dtypes.append(v.dtype)
        tensor_shapes.append(v.shape)
      for i in range(self.hparams.num_shards_per_host):
        with tf.device(
            device_for_host(get_host(self.resolver, self.hparams, host_id))):
          outfeed_tensors = tpu_ops.outfeed_dequeue_tuple(
              dtypes=tensor_dtypes, shapes=tensor_shapes, device_ordinal=i)
          for j, item in enumerate(outfeed_tensors):
            dequeue_ops[j].append(item)
      for j in range(len(outfeed_tensors)):
        dequeue_ops[j] = tf.concat(dequeue_ops[j], axis=0)
      return dequeue_ops

    with self.graph.as_default():
      if self.eval_steps <= 0:
        (self.loss,) = tpu.shard(
            train_loop,
            inputs=[],
            num_shards=self.hparams.num_shards,
            outputs_from_all_shards=False,
        )

      else:
        (
            self.compile_op,
            self.train_eval_op,
        ) = tpu.split_compile_and_shard(
            train_eval_loop,
            inputs=[],
            num_shards=self.hparams.num_shards,
            outputs_from_all_shards=False)

      if self.eval_steps > 0:
        for i in range(0, self.num_hosts):
          self.dequeue_ops.append({})
          host_dequeue_ops = create_dequeue_ops(i)
          for j, dequeue_tenor in enumerate(host_dequeue_ops):
            self.dequeue_ops[i][self.outfeed_names[j]] = dequeue_tenor

      global_initializer = tf.global_variables_initializer()
      local_initializer = tf.local_variables_initializer()
      self.sess.run(global_initializer)
      self.sess.run(local_initializer)

      graph_io.write_graph(
          self.graph.as_graph_def(add_shapes=True), self.hparams.out_dir,
          "graph.pbtxt")
      self.saver = tf.train.Saver()

  def train(self,
            start_step,
            train_steps,
            num_threads=2,
            checkpoint_threads=None):
    """Run the Train loop on the TPU device."""
    tf.logging.info("TrainLowLevelRunner: train for %d steps in total",
                    train_steps)

    def infeed_thread_fn(sess, enqueue_ops):
      assert train_steps % self.iterations == 0
      steps = train_steps // self.iterations
      for _ in range(steps):
        sess.run([enqueue_ops])

    def checkpoint_thread_fn(saver, sess):
      saver.save(sess, self.hparams.out_dir + "/model.ckpt-%d" % (start_step + cur_step))

    infeed_thread = threading.Thread(
        target=infeed_thread_fn, args=(self.sess, self.enqueue_ops))
    infeed_thread.start()

    cur_step = 0
    thread_id = 0
    need_join = False
    if checkpoint_threads is None and num_threads > 0:
      checkpoint_threads = []
      need_join = True
      for i in range(num_threads):
        checkpoint_threads.append(None)
    while cur_step < train_steps:
      start = time.time()
      tf.logging.info("TrainLowLevelRunner: start train step:%d",
                      start_step + cur_step)
      cur_step += self.iterations
      loss = self.sess.run([self.loss])
      tf.logging.info("TrainLowLevelRunner: sess run loss: %s", loss)

      if num_threads > 0:
        if checkpoint_threads[thread_id] is not None:
          checkpoint_threads[thread_id].join()
        checkpoint_threads[thread_id] = threading.Thread(
            target=checkpoint_thread_fn, args=(self.saver, self.sess))
        checkpoint_threads[thread_id].start()
        thread_id += 1
        if thread_id >= num_threads:
          thread_id = 0

      end = time.time()
      tf.logging.info(
          "TrainLowLevelRunner: step time {} sec {} examples/sec".format(
              end - start,
              self.iterations * self.hparams.batch_size / (end - start)))

    infeed_thread.join()

    if need_join:
      for i in range(num_threads):
        if checkpoint_threads[i] is not None:
          checkpoint_threads[i].join()
          checkpoint_threads[i] = None

  def predict(self, checkpoint_path=None):
    """Run the predict loop on the TPU device."""
    if not checkpoint_path:
      checkpoint_path = tf.train.latest_checkpoint(self.hparams.out_dir)

    if self.iterations == 0:
      self.saver.restore(self.sess, checkpoint_path)

    queue = Queue.Queue()
    def dequeue_thread_fn(sess, dequeue_ops, i):
      while True:
        item = queue.get(block=True)
        if item == _STOP:
          return
        self.predictions[i] = sess.run(dequeue_ops)
        queue.task_done()

    dequeue_threads = [None] * self.num_hosts
    for i in range(self.num_hosts):
      dequeue_threads[i] = threading.Thread(
          target=dequeue_thread_fn, args=(self.sess, self.dequeue_ops[i], i))
      dequeue_threads[i].start()

    for step in range(self.eval_steps):
      tf.logging.info("TrainAndEvalLowLevelRunner: start eval step:%d", step)
      for i in range(self.num_hosts):
        queue.put(_ITEM)
      queue.join()
      for j in range(self.num_hosts):
        for i in range(self.hparams.infer_batch_size // self.num_hosts):
          yield {
              key: value[i] for key, value in six.iteritems(self.predictions[j])
          }

    for i in range(self.num_hosts):
      queue.put(_STOP)
    for i in range(self.num_hosts):
      dequeue_threads[i].join()

  def train_and_predict(self):
    """Run the predict loop on the TPU device."""
    self.sess.run([self.compile_op])

    # Train and eval thread.
    def train_eval_thread_fn(sess, train_eval_op):
      tf.logging.info("train_eval_op start")
      sess.run([train_eval_op])

    train_eval_thread = threading.Thread(
        target=train_eval_thread_fn, args=(self.sess, self.train_eval_op))
    train_eval_thread.start()

    # Infeed thread.
    def infeed_thread_fn(sess, train_enqueue_ops, eval_enqueue_ops, eval_init):
      """Start the infeed."""
      time.sleep(150)

      mlp_log.mlperf_print("init_stop", None)
      mlp_log.mlperf_print("run_start", None)
      mlp_log.mlperf_print(
          "block_start",
          None,
          metadata={
              "first_epoch_num": 1,
              "epoch_count": 1
          })

      for i in range(self.hparams.max_train_epochs):
        tf.logging.info("Infeed for epoch: %d", i + 1)
        sess.run(eval_init)
        sess.run([train_enqueue_ops])
        sess.run([eval_enqueue_ops])

    infeed_thread = threading.Thread(
        target=infeed_thread_fn,
        args=(self.sess, self.enqueue_ops, self.eval_enqueue_ops,
              self.eval_dataset_initializer))
    infeed_thread.start()

    if self.eval_steps > 0:
      eval_state = {"run_success": False, "score": 0.0}

      for epoch in range(self.hparams.max_train_epochs):
        predictions = list(self.predict())
        mlp_log.mlperf_print(
            "eval_start", None, metadata={"epoch_num": epoch + 1})
        current_step = epoch * self.iterations

        eval_state["score"] = metric.get_metric(self.hparams, predictions,
                                                current_step)
        tf.logging.info("Score after epoch %d: %f", epoch, eval_state["score"])
        mlp_log.mlperf_print(
            "eval_accuracy",
            eval_state["score"] / 100.0,
            metadata={"epoch_num": epoch + 1})
        mlp_log.mlperf_print(
            "eval_stop", None, metadata={"epoch_num": epoch + 1})
        mlp_log.mlperf_print(
            "block_stop",
            None,
            metadata={
                "first_epoch_num": epoch + 1,
                "epoch_count": 1
            })
        if eval_state["score"] >= self.hparams.target_bleu:
          eval_state["run_success"] = True
          mlp_log.mlperf_print("run_stop", None, metadata={"status": "success"})
          break
        mlp_log.mlperf_print(
            "block_start",
            None,
            metadata={
                "first_epoch_num": epoch + 2,
                "epoch_count": 1
            })

      if not eval_state["run_success"]:
        mlp_log.mlperf_print("run_stop", None, metadata={"status": "abort"})

    infeed_thread.join()
    train_eval_thread.join()

    if self.eval_steps > 0:
      return eval_state["score"], current_step
    else:
      return None, None
